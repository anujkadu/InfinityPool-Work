{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3VMp1+ubNeEFwYuZkdb2W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anujkadu/InfinityPool-Work/blob/main/Infinity_Pool_TensorFlow_Manual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Dataset (will default a loan)"
      ],
      "metadata": {
        "id": "s4UdHQG4J8t6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "CJvNI0OL0Hhx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Generate synthetic financial dataset\n",
        "np.random.seed(42)\n",
        "N = 1000\n",
        "df = pd.DataFrame({\n",
        "    'age': np.random.randint(21, 60, N),\n",
        "    'income': np.random.randint(20000, 150000, N),\n",
        "    'credit_score': np.random.randint(300, 900, N),\n",
        "    'loan_amount': np.random.randint(50000, 1000000, N),\n",
        "    'loan_term': np.random.choice([12, 24, 36, 48, 60], N),\n",
        "    'past_defaults': np.random.poisson(0.5, N)\n",
        "})\n",
        "df['will_default'] = (\n",
        "    (df['credit_score'] < 600).astype(int) |\n",
        "    ((df['loan_amount'] / df['income']) > 10).astype(int) |\n",
        "    (df['past_defaults'] > 2).astype(int)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert to tensorflow compatible format"
      ],
      "metadata": {
        "id": "-zT6Nc7HKA6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw = df.drop(\"will_default\", axis=1).values\n",
        "y = df[\"will_default\"].values\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X_raw)\n",
        "\n",
        "X_tensor = tf.constant(X, dtype=tf.float32)\n",
        "y_tensor = tf.constant(y.reshape(-1, 1), dtype=tf.float32)"
      ],
      "metadata": {
        "id": "ubZWZ06Q0QI3"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manually declare weights for each neuron of our model"
      ],
      "metadata": {
        "id": "tZ8muje8KE-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = tf.Variable([\n",
        "    [0.05, 0.1, 0.2, 0.0, 0.1, 0.3, 0.0, 0.15],\n",
        "    [0.0, 0.0, 0.1, 0.2, 0.0, 0.2, 0.1, 0.1],\n",
        "    [0.2, 0.2, 0.0, 0.1, 0.3, 0.0, 0.1, 0.0],\n",
        "    [0.1, 0.0, 0.0, 0.3, 0.1, 0.0, 0.2, 0.05],\n",
        "    [0.05, 0.1, 0.1, 0.1, 0.05, 0.1, 0.0, 0.0],\n",
        "    [0.3, 0.2, 0.1, 0.0, 0.0, 0.1, 0.3, 0.2]\n",
        "], dtype=tf.float32)\n",
        "b1 = tf.Variable([0.1, 0.2, 0.0, -0.1, 0.1, 0.0, 0.05, -0.05], dtype=tf.float32)\n",
        "\n",
        "# Second hidden layer (8 → 4)\n",
        "W2 = tf.Variable([\n",
        "    [0.10, 0.20, 0.00, 0.10],\n",
        "    [0.00, 0.10, 0.20, 0.00],\n",
        "    [0.20, 0.10, 0.10, 0.00],\n",
        "    [0.10, 0.00, 0.00, 0.30],\n",
        "    [0.10, 0.00, 0.30, 0.10],\n",
        "    [0.00, 0.20, 0.10, 0.00],\n",
        "    [0.20, 0.10, 0.00, 0.10],\n",
        "    [0.00, 0.00, 0.10, 0.20]\n",
        "], dtype=tf.float32)\n",
        "b2 = tf.Variable([0.00, 0.10, -0.05, 0.20], dtype=tf.float32)\n",
        "\n",
        "# Output layer (4 → 1)\n",
        "W3 = tf.Variable([[0.2], [0.1], [-0.1], [0.3]], dtype=tf.float32)\n",
        "b3 = tf.Variable([0.1], dtype=tf.float32)"
      ],
      "metadata": {
        "id": "JGsvxuMH0Twm"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate linear transformation and use activation function"
      ],
      "metadata": {
        "id": "ODTApj4YKRSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z1 = tf.matmul(X_tensor, W1) + b1\n",
        "A1 = tf.nn.relu(Z1)\n",
        "\n",
        "Z2 = tf.matmul(A1, W2) + b2\n",
        "A2 = tf.nn.relu(Z2)\n",
        "\n",
        "Z3 = tf.matmul(A2, W3) + b3\n",
        "A3 = tf.nn.sigmoid(Z3)  # final output: probability\n",
        "\n",
        "print(\"Predicted Probabilities (first 5):\")\n",
        "print(A3.numpy()[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVZJxXc90dAY",
        "outputId": "28880d73-7785-4975-ee10-a0ba51425807"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Probabilities (first 5):\n",
            "[[0.57750833]\n",
            " [0.571759  ]\n",
            " [0.5744479 ]\n",
            " [0.5485885 ]\n",
            " [0.5649627 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Declare optimizer and loss function"
      ],
      "metadata": {
        "id": "fXeDd8IEKL0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "\n",
        "# epsilon = 1e-7  # for numerical stability to prevent log(0)\n",
        "# loss_fn = tf.reduce_mean(\n",
        "#     -y_tensor * tf.math.log(A3 + epsilon) - (1 - y_tensor) * tf.math.log(1 - A3 + epsilon)\n",
        "# )\n"
      ],
      "metadata": {
        "id": "VQZflSd70W19"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Loss and Accuracy before training"
      ],
      "metadata": {
        "id": "DR2IwrDeKcER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(y_tensor, A3)\n",
        "y_pred_before = tf.cast(A3 > 0.5, dtype=tf.int32)\n",
        "y_true = tf.cast(y_tensor, dtype=tf.int32)\n",
        "accuracy_before = tf.reduce_mean(tf.cast(tf.equal(y_pred_before, y_true), tf.float32))\n",
        "\n",
        "print(f\"Loss: {loss.numpy():.4f}\")\n",
        "#print(\"Loss = \",loss_fn)\n",
        "print(f\"Accuracy Before Update: {accuracy_before.numpy() * 100:.4f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWA-iCYb0gHR",
        "outputId": "5fe575d9-977a-4a8a-951b-fdf03467d29a"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6682\n",
            "Accuracy Before Update: 63.8000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample prediction before training"
      ],
      "metadata": {
        "id": "-iPnWr7DKgQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_customer = pd.DataFrame([{\n",
        "    'age': 29,\n",
        "    'income': 25000,\n",
        "    'credit_score': 490,\n",
        "    'loan_amount': 450000,\n",
        "    'loan_term': 60,\n",
        "    'past_defaults': 3\n",
        "}])\n",
        "\n",
        "# Normalize\n",
        "new_customer_scaled = scaler.transform(new_customer)\n",
        "X_new = tf.constant(new_customer_scaled, dtype=tf.float32)\n",
        "\n",
        "# Manual forward pass BEFORE training\n",
        "Z1 = tf.matmul(X_new, W1) + b1\n",
        "A1 = tf.nn.relu(Z1)\n",
        "Z2 = tf.matmul(A1, W2) + b2\n",
        "A2 = tf.nn.relu(Z2)\n",
        "Z3 = tf.matmul(A2, W3) + b3\n",
        "A3 = tf.nn.sigmoid(Z3)\n",
        "\n",
        "print(\"BEFORE Training → Probability of default:\", A3.numpy()[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luHWLTON7bkQ",
        "outputId": "09433952-d56a-480d-8bd0-1ab142a56f60"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEFORE Training → Probability of default: 0.5693356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Gradients"
      ],
      "metadata": {
        "id": "glg9UMJiKlgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "tf.matmul: matrix multiplication\n",
        "\n",
        "tf.nn.relu: zeroes out negatives (adds non-linearity)\n",
        "\n",
        "tf.nn.sigmoid: converts raw score into probability between 0 and 1"
      ],
      "metadata": {
        "id": "AiGJn7caThfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape() as tape:\n",
        "    Z1 = tf.matmul(X_tensor, W1) + b1\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    Z2 = tf.matmul(A1, W2) + b2\n",
        "    A2 = tf.nn.relu(Z2)\n",
        "    Z3 = tf.matmul(A2, W3) + b3\n",
        "    A3 = tf.nn.sigmoid(Z3)\n",
        "    loss = loss_fn(y_tensor, A3)\n",
        "\n",
        "# Compute gradients\n",
        "grads = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
        "print(\"Gradients calculated for all weights and biases.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW9M4Sul0lBs",
        "outputId": "24bd4747-be3e-4bf1-b0a5-3b158619dcf6"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients calculated for all weights and biases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Sample Epoch"
      ],
      "metadata": {
        "id": "iva_eXSOKuwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply gradients to update weights\n",
        "optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2, W3, b3]))\n",
        "\n",
        "# Rerun forward pass to see new predictions\n",
        "Z1 = tf.matmul(X_tensor, W1) + b1\n",
        "A1 = tf.nn.relu(Z1)\n",
        "Z2 = tf.matmul(A1, W2) + b2\n",
        "A2 = tf.nn.relu(Z2)\n",
        "Z3 = tf.matmul(A2, W3) + b3\n",
        "A3 = tf.nn.sigmoid(Z3)\n",
        "\n",
        "# Accuracy after update\n",
        "y_pred = tf.cast(A3 > 0.5, dtype=tf.int32)\n",
        "y_true = tf.cast(y_tensor, dtype=tf.int32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
        "\n",
        "print(\"Predicted Probabilities After Update (first 5):\")\n",
        "print(A3.numpy()[:5])\n",
        "print(f\"Accuracy After Update: {accuracy.numpy() * 100:.4f}%\")\n",
        "\n",
        "# Print updated weights\n",
        "print(\"\\nUpdated Weights:\")\n",
        "print(\"W1:\\n\", W1.numpy())\n",
        "print(\"W2:\\n\", W2.numpy())\n",
        "print(\"W3:\\n\", W3.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dntOkjrM0nA7",
        "outputId": "dee4181a-1270-4871-daff-baf1c4f8951e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Probabilities After Update (first 5):\n",
            "[[0.6092458 ]\n",
            " [0.59981   ]\n",
            " [0.6042871 ]\n",
            " [0.56509566]\n",
            " [0.59017265]]\n",
            "Accuracy After Update: 63.8000%\n",
            "\n",
            "Updated Weights:\n",
            "W1:\n",
            " [[ 0.06997666  0.08016128  0.21995932  0.01998376  0.11991894  0.31983873\n",
            "   0.01997955  0.16996722]\n",
            " [-0.01859707  0.0131257   0.0823295   0.18050514 -0.01583617  0.1868743\n",
            "   0.08123872  0.09125167]\n",
            " [ 0.18001519  0.21989517 -0.01997356  0.08000957  0.28005266 -0.01989518\n",
            "   0.08001331 -0.01997813]\n",
            " [ 0.11998644 -0.01990647  0.01997641  0.3199913   0.11995304  0.01990647\n",
            "   0.21998812  0.06998056]\n",
            " [ 0.06997854  0.08014826  0.1199626   0.11998606  0.0699255   0.11985174\n",
            "   0.0199812   0.01996941]\n",
            " [ 0.3199202   0.18054435  0.11986087  0.01994788  0.01972388  0.11945565\n",
            "   0.3199301   0.21988851]]\n",
            "W2:\n",
            " [[ 0.1199696   0.21993947 -0.01993947  0.11997966]\n",
            " [ 0.01997864  0.1199575   0.1800425   0.0199857 ]\n",
            " [ 0.21997571  0.11995166  0.08004835  0.01998375]\n",
            " [ 0.1199688   0.01993786 -0.01993786  0.31997913]\n",
            " [ 0.11989468  0.01979066  0.28020936  0.1199296 ]\n",
            " [ 0.01998117  0.21996255  0.08003746  0.01998739]\n",
            " [ 0.21997869  0.11995759 -0.01995758  0.11998573]\n",
            " [ 0.0199566   0.01991356  0.08008644  0.219971  ]]\n",
            "W3:\n",
            " [[ 0.2199919 ]\n",
            " [ 0.11999633]\n",
            " [-0.08001835]\n",
            " [ 0.3199972 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running multiple epochs"
      ],
      "metadata": {
        "id": "z0jLg2XBMCyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass\n",
        "        Z1 = tf.matmul(X_tensor, W1) + b1\n",
        "        A1 = tf.nn.relu(Z1)\n",
        "        Z2 = tf.matmul(A1, W2) + b2\n",
        "        A2 = tf.nn.relu(Z2)\n",
        "        Z3 = tf.matmul(A2, W3) + b3\n",
        "        A3 = tf.nn.sigmoid(Z3)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(y_tensor, A3)\n",
        "\n",
        "    # Compute gradients\n",
        "    grads = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
        "    # Update weights\n",
        "    optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2, W3, b3]))\n",
        "\n",
        "    # Compute accuracy\n",
        "    y_pred = tf.cast(A3 > 0.5, dtype=tf.int32)\n",
        "    y_true = tf.cast(y_tensor, dtype=tf.int32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} → Loss: {loss.numpy():.4f} | Accuracy: {accuracy.numpy() * 100:.2f}%\")\n",
        "    # print(\"\\nUpdated Weights:\")\n",
        "    # print(\"W1:\\n\", W1.numpy())\n",
        "    # print(\"W2:\\n\", W2.numpy())\n",
        "    # print(\"W3:\\n\", W3.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CqH-xoL0pF-",
        "outputId": "374a95ae-8f1d-4916-8ce9-959a7b4b06cb"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 → Loss: 0.6624 | Accuracy: 63.80%\n",
            "Epoch 010 → Loss: 0.6448 | Accuracy: 63.80%\n",
            "Epoch 020 → Loss: 0.6025 | Accuracy: 63.80%\n",
            "Epoch 030 → Loss: 0.5138 | Accuracy: 75.10%\n",
            "Epoch 040 → Loss: 0.4042 | Accuracy: 82.40%\n",
            "Epoch 050 → Loss: 0.3186 | Accuracy: 85.90%\n",
            "Epoch 060 → Loss: 0.2903 | Accuracy: 86.20%\n",
            "Epoch 070 → Loss: 0.2871 | Accuracy: 86.60%\n",
            "Epoch 080 → Loss: 0.2872 | Accuracy: 86.00%\n",
            "Epoch 090 → Loss: 0.2863 | Accuracy: 86.60%\n",
            "Epoch 100 → Loss: 0.2858 | Accuracy: 86.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction based on fine tuned model"
      ],
      "metadata": {
        "id": "Xo1oXKmzMF02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_customer = pd.DataFrame([{\n",
        "    'age': 29,\n",
        "    'income': 25000,\n",
        "    'credit_score': 490,\n",
        "    'loan_amount': 450000,\n",
        "    'loan_term': 60,\n",
        "    'past_defaults': 3\n",
        "}])\n",
        "\n",
        "# Normalize\n",
        "new_customer_scaled = scaler.transform(new_customer)\n",
        "X_new = tf.constant(new_customer_scaled, dtype=tf.float32)\n",
        "\n",
        "# Manual forward pass BEFORE training\n",
        "Z1 = tf.matmul(X_new, W1) + b1\n",
        "A1 = tf.nn.relu(Z1)\n",
        "Z2 = tf.matmul(A1, W2) + b2\n",
        "A2 = tf.nn.relu(Z2)\n",
        "Z3 = tf.matmul(A2, W3) + b3\n",
        "A3 = tf.nn.sigmoid(Z3)\n",
        "\n",
        "print(\"BEFORE Training → Probability of default:\", A3.numpy()[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swQpmjOq7l1T",
        "outputId": "4bc09e1a-ef34-46aa-db5f-cd7f5e41d9a8"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEFORE Training → Probability of default: 0.9992362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}