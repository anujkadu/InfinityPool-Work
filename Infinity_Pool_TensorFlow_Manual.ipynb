{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNe34vGxNyJM3PhwiZpzf31",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anujkadu/InfinityPool-Work/blob/main/Infinity_Pool_TensorFlow_Manual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Dataset (will default a loan)"
      ],
      "metadata": {
        "id": "s4UdHQG4J8t6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "CJvNI0OL0Hhx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Generate synthetic financial dataset\n",
        "np.random.seed(42)\n",
        "N = 1000\n",
        "df = pd.DataFrame({\n",
        "    'age': np.random.randint(21, 60, N),\n",
        "    'income': np.random.randint(20000, 150000, N),\n",
        "    'credit_score': np.random.randint(300, 900, N),\n",
        "    'loan_amount': np.random.randint(50000, 1000000, N),\n",
        "    'loan_term': np.random.choice([12, 24, 36, 48, 60], N),\n",
        "    'past_defaults': np.random.poisson(0.5, N)\n",
        "})\n",
        "df['will_default'] = (\n",
        "    (df['credit_score'] < 600).astype(int) |\n",
        "    ((df['loan_amount'] / df['income']) > 10).astype(int) |\n",
        "    (df['past_defaults'] > 2).astype(int)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert to tensorflow compatible format"
      ],
      "metadata": {
        "id": "-zT6Nc7HKA6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw = df.drop(\"will_default\", axis=1).values\n",
        "y = df[\"will_default\"].values\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X_raw)\n",
        "\n",
        "X_tensor = tf.constant(X, dtype=tf.float32)\n",
        "y_tensor = tf.constant(y.reshape(-1, 1), dtype=tf.float32)"
      ],
      "metadata": {
        "id": "ubZWZ06Q0QI3"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manually declare weights for each neuron of our model"
      ],
      "metadata": {
        "id": "tZ8muje8KE-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = tf.Variable([\n",
        "    [0.05, 0.1, 0.2, 0.0, 0.1, 0.3, 0.0, 0.15],\n",
        "    [0.0, 0.0, 0.1, 0.2, 0.0, 0.2, 0.1, 0.1],\n",
        "    [0.2, 0.2, 0.0, 0.1, 0.3, 0.0, 0.1, 0.0],\n",
        "    [0.1, 0.0, 0.0, 0.3, 0.1, 0.0, 0.2, 0.05],\n",
        "    [0.05, 0.1, 0.1, 0.1, 0.05, 0.1, 0.0, 0.0],\n",
        "    [0.3, 0.2, 0.1, 0.0, 0.0, 0.1, 0.3, 0.2]\n",
        "], dtype=tf.float32)\n",
        "b1 = tf.Variable([0.1, 0.2, 0.0, -0.1, 0.1, 0.0, 0.05, -0.05], dtype=tf.float32)\n",
        "\n",
        "# Second hidden layer (8 → 4)\n",
        "W2 = tf.Variable([\n",
        "    [0.10, 0.20, 0.00, 0.10],\n",
        "    [0.00, 0.10, 0.20, 0.00],\n",
        "    [0.20, 0.10, 0.10, 0.00],\n",
        "    [0.10, 0.00, 0.00, 0.30],\n",
        "    [0.10, 0.00, 0.30, 0.10],\n",
        "    [0.00, 0.20, 0.10, 0.00],\n",
        "    [0.20, 0.10, 0.00, 0.10],\n",
        "    [0.00, 0.00, 0.10, 0.20]\n",
        "], dtype=tf.float32)\n",
        "b2 = tf.Variable([0.00, 0.10, -0.05, 0.20], dtype=tf.float32)\n",
        "\n",
        "# Output layer (4 → 1)\n",
        "W3 = tf.Variable([[0.2], [0.1], [-0.1], [0.3]], dtype=tf.float32)\n",
        "b3 = tf.Variable([0.1], dtype=tf.float32)"
      ],
      "metadata": {
        "id": "JGsvxuMH0Twm"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Declare optimizer and loss function"
      ],
      "metadata": {
        "id": "fXeDd8IEKL0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()"
      ],
      "metadata": {
        "id": "VQZflSd70W19"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate linear transformation and use activation function"
      ],
      "metadata": {
        "id": "ODTApj4YKRSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z1 = tf.matmul(X_tensor, W1) + b1\n",
        "A1 = tf.nn.relu(Z1)\n",
        "\n",
        "Z2 = tf.matmul(A1, W2) + b2\n",
        "A2 = tf.nn.relu(Z2)\n",
        "\n",
        "Z3 = tf.matmul(A2, W3) + b3\n",
        "A3 = tf.nn.sigmoid(Z3)  # final output: probability\n",
        "\n",
        "print(\"Predicted Probabilities (first 5):\")\n",
        "print(A3.numpy()[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVZJxXc90dAY",
        "outputId": "477bc2f6-0966-41a6-bdde-f15ee5fc7c66"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Probabilities (first 5):\n",
            "[[0.57750833]\n",
            " [0.571759  ]\n",
            " [0.5744479 ]\n",
            " [0.5485885 ]\n",
            " [0.5649627 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Loss and Accuracy before training"
      ],
      "metadata": {
        "id": "DR2IwrDeKcER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(y_tensor, A3)\n",
        "y_pred_before = tf.cast(A3 > 0.5, dtype=tf.int32)\n",
        "y_true = tf.cast(y_tensor, dtype=tf.int32)\n",
        "accuracy_before = tf.reduce_mean(tf.cast(tf.equal(y_pred_before, y_true), tf.float32))\n",
        "\n",
        "print(f\"Loss: {loss.numpy():.4f}\")\n",
        "print(f\"Accuracy Before Update: {accuracy_before.numpy() * 100:.4f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWA-iCYb0gHR",
        "outputId": "95708cdc-b14b-458b-f2c4-860118ffed92"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6682\n",
            "Accuracy Before Update: 63.8000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample prediction before training"
      ],
      "metadata": {
        "id": "-iPnWr7DKgQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_customer = pd.DataFrame([{\n",
        "    'age': 29,\n",
        "    'income': 25000,\n",
        "    'credit_score': 490,\n",
        "    'loan_amount': 450000,\n",
        "    'loan_term': 60,\n",
        "    'past_defaults': 3\n",
        "}])\n",
        "\n",
        "# Normalize\n",
        "new_customer_scaled = scaler.transform(new_customer)\n",
        "X_new = tf.constant(new_customer_scaled, dtype=tf.float32)\n",
        "\n",
        "# Manual forward pass BEFORE training\n",
        "Z1 = tf.matmul(X_new, W1) + b1\n",
        "A1 = tf.nn.relu(Z1)\n",
        "Z2 = tf.matmul(A1, W2) + b2\n",
        "A2 = tf.nn.relu(Z2)\n",
        "Z3 = tf.matmul(A2, W3) + b3\n",
        "A3 = tf.nn.sigmoid(Z3)\n",
        "\n",
        "print(\"BEFORE Training → Probability of default:\", A3.numpy()[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luHWLTON7bkQ",
        "outputId": "6004344f-fee7-49d6-9298-2953b888dc41"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEFORE Training → Probability of default: 0.5693356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Gradients"
      ],
      "metadata": {
        "id": "glg9UMJiKlgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "tf.matmul: matrix multiplication\n",
        "\n",
        "tf.nn.relu: zeroes out negatives (adds non-linearity)\n",
        "\n",
        "tf.nn.sigmoid: converts raw score into probability between 0 and 1"
      ],
      "metadata": {
        "id": "AiGJn7caThfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape() as tape:\n",
        "    Z1 = tf.matmul(X_tensor, W1) + b1\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    Z2 = tf.matmul(A1, W2) + b2\n",
        "    A2 = tf.nn.relu(Z2)\n",
        "    Z3 = tf.matmul(A2, W3) + b3\n",
        "    A3 = tf.nn.sigmoid(Z3)\n",
        "    loss = loss_fn(y_tensor, A3)\n",
        "\n",
        "# Compute gradients\n",
        "grads = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
        "print(\"Gradients calculated for all weights and biases.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW9M4Sul0lBs",
        "outputId": "bfd4e8ca-16e8-42dd-f096-ed075dd2af6e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients calculated for all weights and biases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Sample Epoch"
      ],
      "metadata": {
        "id": "iva_eXSOKuwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply gradients to update weights\n",
        "optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2, W3, b3]))\n",
        "\n",
        "# Rerun forward pass to see new predictions\n",
        "Z1 = tf.matmul(X_tensor, W1) + b1\n",
        "A1 = tf.nn.relu(Z1)\n",
        "Z2 = tf.matmul(A1, W2) + b2\n",
        "A2 = tf.nn.relu(Z2)\n",
        "Z3 = tf.matmul(A2, W3) + b3\n",
        "A3 = tf.nn.sigmoid(Z3)\n",
        "\n",
        "# Accuracy after update\n",
        "y_pred = tf.cast(A3 > 0.5, dtype=tf.int32)\n",
        "y_true = tf.cast(y_tensor, dtype=tf.int32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
        "\n",
        "print(\"Predicted Probabilities After Update (first 5):\")\n",
        "print(A3.numpy()[:5])\n",
        "print(f\"Accuracy After Update: {accuracy.numpy() * 100:.4f}%\")\n",
        "\n",
        "# Print updated weights\n",
        "print(\"\\nUpdated Weights:\")\n",
        "print(\"W1:\\n\", W1.numpy())\n",
        "print(\"W2:\\n\", W2.numpy())\n",
        "print(\"W3:\\n\", W3.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dntOkjrM0nA7",
        "outputId": "9697ff31-f125-45b4-d140-ab28f3374c0b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Probabilities After Update (first 5):\n",
            "[[0.59297323]\n",
            " [0.58545554]\n",
            " [0.588917  ]\n",
            " [0.55651724]\n",
            " [0.5771877 ]]\n",
            "Accuracy After Update: 63.8000%\n",
            "\n",
            "Updated Weights:\n",
            "W1:\n",
            " [[ 0.05998636  0.09009433  0.20997621  0.00999052  0.10995258  0.30990568\n",
            "   0.00998805  0.15998083]\n",
            " [-0.00918652  0.00617352  0.0913417   0.19029479 -0.00763409  0.19382648\n",
            "   0.09071912  0.09605084]\n",
            " [ 0.19000886  0.20993868 -0.00998455  0.09000558  0.2900308  -0.00993868\n",
            "   0.09000777 -0.00998722]\n",
            " [ 0.10999209 -0.00994529  0.00998621  0.30999494  0.10997254  0.00994529\n",
            "   0.20999308  0.05998865]\n",
            " [ 0.05998746  0.09008672  0.10997813  0.10999187  0.05995642  0.10991328\n",
            "   0.00998902  0.00998212]\n",
            " [ 0.30995333  0.19031757  0.10991862  0.00996951  0.00983862  0.10968243\n",
            "   0.3099591   0.20993479]]\n",
            "W2:\n",
            " [[ 0.10998223  0.20996459 -0.00996459  0.10998812]\n",
            " [ 0.00998752  0.10997514  0.19002485  0.00999166]\n",
            " [ 0.20998581  0.10997172  0.09002828  0.00999051]\n",
            " [ 0.10998176  0.00996365 -0.00996365  0.3099878 ]\n",
            " [ 0.10993839  0.0098776   0.29012242  0.10995882]\n",
            " [ 0.009989    0.2099781   0.09002191  0.00999264]\n",
            " [ 0.20998755  0.1099752  -0.00997519  0.10999168]\n",
            " [ 0.00997462  0.00994943  0.09005057  0.20998305]]\n",
            "W3:\n",
            " [[ 0.20999528]\n",
            " [ 0.10999788]\n",
            " [-0.09001072]\n",
            " [ 0.3099984 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running multiple epochs"
      ],
      "metadata": {
        "id": "z0jLg2XBMCyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass\n",
        "        Z1 = tf.matmul(X_tensor, W1) + b1\n",
        "        A1 = tf.nn.relu(Z1)\n",
        "        Z2 = tf.matmul(A1, W2) + b2\n",
        "        A2 = tf.nn.relu(Z2)\n",
        "        Z3 = tf.matmul(A2, W3) + b3\n",
        "        A3 = tf.nn.sigmoid(Z3)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(y_tensor, A3)\n",
        "\n",
        "    # Compute gradients\n",
        "    grads = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
        "    # Update weights\n",
        "    optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2, W3, b3]))\n",
        "\n",
        "    # Compute accuracy\n",
        "    y_pred = tf.cast(A3 > 0.5, dtype=tf.int32)\n",
        "    y_true = tf.cast(y_tensor, dtype=tf.int32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} → Loss: {loss.numpy():.4f} | Accuracy: {accuracy.numpy() * 100:.2f}%\")\n",
        "    # print(\"\\nUpdated Weights:\")\n",
        "    # print(\"W1:\\n\", W1.numpy())\n",
        "    # print(\"W2:\\n\", W2.numpy())\n",
        "    # print(\"W3:\\n\", W3.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CqH-xoL0pF-",
        "outputId": "bdd11227-b6fa-483e-fd3e-e15fd54537c2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 → Loss: 0.6651 | Accuracy: 63.80%\n",
            "Epoch 010 → Loss: 0.6482 | Accuracy: 63.80%\n",
            "Epoch 020 → Loss: 0.6134 | Accuracy: 63.80%\n",
            "Epoch 030 → Loss: 0.5301 | Accuracy: 73.70%\n",
            "Epoch 040 → Loss: 0.4206 | Accuracy: 81.60%\n",
            "Epoch 050 → Loss: 0.3278 | Accuracy: 86.30%\n",
            "Epoch 060 → Loss: 0.2919 | Accuracy: 86.10%\n",
            "Epoch 070 → Loss: 0.2871 | Accuracy: 86.40%\n",
            "Epoch 080 → Loss: 0.2873 | Accuracy: 86.20%\n",
            "Epoch 090 → Loss: 0.2864 | Accuracy: 86.40%\n",
            "Epoch 100 → Loss: 0.2859 | Accuracy: 86.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction based on fine tuned model"
      ],
      "metadata": {
        "id": "Xo1oXKmzMF02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_customer = pd.DataFrame([{\n",
        "    'age': 29,\n",
        "    'income': 25000,\n",
        "    'credit_score': 490,\n",
        "    'loan_amount': 450000,\n",
        "    'loan_term': 60,\n",
        "    'past_defaults': 3\n",
        "}])\n",
        "\n",
        "# Normalize\n",
        "new_customer_scaled = scaler.transform(new_customer)\n",
        "X_new = tf.constant(new_customer_scaled, dtype=tf.float32)\n",
        "\n",
        "# Manual forward pass BEFORE training\n",
        "Z1 = tf.matmul(X_new, W1) + b1\n",
        "A1 = tf.nn.relu(Z1)\n",
        "Z2 = tf.matmul(A1, W2) + b2\n",
        "A2 = tf.nn.relu(Z2)\n",
        "Z3 = tf.matmul(A2, W3) + b3\n",
        "A3 = tf.nn.sigmoid(Z3)\n",
        "\n",
        "print(\"BEFORE Training → Probability of default:\", A3.numpy()[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swQpmjOq7l1T",
        "outputId": "33dc39b0-3a90-45ae-90d8-38177757136a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEFORE Training → Probability of default: 0.9992848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}